{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Set, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed 고정\n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED']=str(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.10\n",
      "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
      "     ------------------------------------- 660.2/660.2 kB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers==2.10) (1.24.2)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n",
      "     ---------------------------------------- 81.1/81.1 kB 4.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers==2.10) (3.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers==2.10) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers==2.10) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers==2.10) (2023.3.23)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers==2.10) (0.1.99)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from tqdm>=4.27->transformers==2.10) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->transformers==2.10) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->transformers==2.10) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->transformers==2.10) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->transformers==2.10) (2022.12.7)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from sacremoses->transformers==2.10) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from sacremoses->transformers==2.10) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from sacremoses->transformers==2.10) (1.2.0)\n",
      "Building wheels for collected packages: tokenizers, sacremoses\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895252 sha256=f8a251a9016b60fa94793191268b4ac37e025f705d8df13fb91738094ab76d39\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\11\\75\\c6\\a82d827a00df823caf211262900d2c024f5b3a775b82b45230\n",
      "Successfully built sacremoses\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [46 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.20.0-cp311-cp311-win_amd64.whl (736 kB)\n",
      "     ------------------------------------- 736.6/736.6 kB 22.7 MB/s eta 0:00:00\n",
      "Collecting typeguard<3.0.0,>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-addons) (23.0)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
