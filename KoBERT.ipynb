{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Set, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed 고정\n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED']=str(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_review=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\KoBERT 감성분류\\data\\food_review.csv')\n",
    "kakao=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\KoBERT 감성분류\\data\\kakao_review_0to500.csv',encoding='cp949')\n",
    "thoure=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\KoBERT 감성분류\\data\\1500개_리뷰_label.CSV',encoding='cp949')\n",
    "hong=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\KoBERT 감성분류\\data\\홍대_맛집_리뷰_데이터_ver5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>맛있는데 고기 양이 조금 적을 수도.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>점심시간만되면 문전성시인곳. 혼자와서 먹는사람도 많음. 진짜 꼬수운 청국장 생각날때...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>맛있어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>반찬이 부실하네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>맛있어요</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     4                               맛있는데 고기 양이 조금 적을 수도.\n",
       "0  4.0  점심시간만되면 문전성시인곳. 혼자와서 먹는사람도 많음. 진짜 꼬수운 청국장 생각날때...\n",
       "1  4.5                                               맛있어요\n",
       "2  5.0                                                  .\n",
       "3  4.5                                          반찬이 부실하네요\n",
       "4  4.0                                               맛있어요"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\KoBERT 감성분류\\data\\restaurant_review.txt',sep='\\t')\n",
    "rr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_review.drop('score',axis=1,inplace=True)\n",
    "kakao.drop(['store_id','kakao_nickname','kakao_date'],axis=1,inplace=True)\n",
    "thoure=thoure[['review','label']]\n",
    "hong=hong[['user_rating','comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>친절하시고 깔끔하고 좋았습니다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>조용하고 고기도 굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>갈비탕과 냉면, 육회비빔밥이 맛있습니다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대체적으로 만족하나\\n와인의 구성이 살짝 아쉬움</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>고기도 맛있고 서비스는 더 최고입니다~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       review  y\n",
       "0            친절하시고 깔끔하고 좋았습니다  1\n",
       "1                  조용하고 고기도 굿  1\n",
       "2      갈비탕과 냉면, 육회비빔밥이 맛있습니다.  1\n",
       "3  대체적으로 만족하나\\n와인의 구성이 살짝 아쉬움  1\n",
       "4       고기도 맛있고 서비스는 더 최고입니다~  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kakao_content</th>\n",
       "      <th>kakao_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>차돌쭈꾸미 새로생겨서 먹어봤는디 정말맛있어요존맛탱구리구리 오늘도 아주 잘먹구갑니다 ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>진짜 맛집 근처 다른 해물찜들 먹다가 가게가 보여서 들어와 봤는데 진짜 맛있네요벌써...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>처음 방문했는데 생각보다 많이 달라서 당황했습니다 천안본점에는 밑반찬만 6가지 정도...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>지금껏 먹어본 해물찜과는 정말 다르게 너무 맛있어서 먹고 포장까지 해왔습니다새우찜도...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>생고기 맛 좋아서 추천합니다</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       kakao_content  kakao_star\n",
       "0  차돌쭈꾸미 새로생겨서 먹어봤는디 정말맛있어요존맛탱구리구리 오늘도 아주 잘먹구갑니다 ...         5.0\n",
       "1  진짜 맛집 근처 다른 해물찜들 먹다가 가게가 보여서 들어와 봤는데 진짜 맛있네요벌써...         5.0\n",
       "2  처음 방문했는데 생각보다 많이 달라서 당황했습니다 천안본점에는 밑반찬만 6가지 정도...         1.0\n",
       "3  지금껏 먹어본 해물찜과는 정말 다르게 너무 맛있어서 먹고 포장까지 해왔습니다새우찜도...         5.0\n",
       "4                                    생고기 맛 좋아서 추천합니다         5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>주차시설좋고 넓고 깔끔하고 좋아요. 집 근처라 더욱 좋네요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아포카토 넘넘 맛있어요??</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>넓고 쾌적한 실내가 좋아요. 커피도 굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>매장이 넓어서 좋아요!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>친절하시고 음료도 맛있어요 :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             review  label\n",
       "0  주차시설좋고 넓고 깔끔하고 좋아요. 집 근처라 더욱 좋네요      1\n",
       "1                    아포카토 넘넘 맛있어요??      1\n",
       "2             넓고 쾌적한 실내가 좋아요. 커피도 굿      1\n",
       "3                      매장이 넓어서 좋아요!      1\n",
       "4                 친절하시고 음료도 맛있어요 :)      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>❤️❤️❤️❤️❤️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>오래 해주세요🙏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>웨이팅해서 먹을 맛은 아니에요… 닭보쌈은 보통이었고 비빔국수라고 해서 당연히 국물 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_rating                                            comment\n",
       "0          5.0                                                NaN\n",
       "1          5.0                                         ❤️❤️❤️❤️❤️\n",
       "2          5.0                                           오래 해주세요🙏\n",
       "3          3.0  웨이팅해서 먹을 맛은 아니에요… 닭보쌈은 보통이었고 비빔국수라고 해서 당연히 국물 ...\n",
       "4          5.0       와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(food_review.head())\n",
    "display(kakao.head())\n",
    "display(thoure.head())\n",
    "display(hong.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null값 없애기\n",
    "kakao.dropna(axis=0,inplace=True)\n",
    "hong.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 545 entries, 0 to 544\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  545 non-null    object\n",
      " 1   label   545 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 965 entries, 0 to 964\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   kakao_content  965 non-null    object \n",
      " 1   label          965 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 15.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1574 entries, 0 to 1573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  1574 non-null   object\n",
      " 1   label   1574 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2201 entries, 0 to 2200\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   comment  2201 non-null   object \n",
      " 1   label    2201 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 34.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(food_review.info())\n",
    "display(kakao.info())\n",
    "display(thoure.info())\n",
    "display(hong.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5,4는 1의 긍정레이블, 1,2는 부정레이블, 3은 삭제\n",
    "def label_mapping(labels):\n",
    "    return [1 if label in [5, 4, 4.5] else 0 if label in [1, 2,0.5,2.5,1.5] else label for label in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kakao_new_labels=label_mapping(kakao['kakao_star'])\n",
    "new_labels=label_mapping(hong['user_rating'])\n",
    "rr_labels=label_mapping(rr['4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "kakao['label']=kakao_new_labels\n",
    "hong['label']=new_labels\n",
    "rr['label']=rr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_rating</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>❤️❤️❤️❤️❤️</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>오래 해주세요🙏</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>오늘의 메뉴 너무 좋아요. 어쩜 갖가지 닭요리를 그렇게 잘하시죠?!! 이런 리뷰 잘...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>여기서 밥먹고 둘다 장염걸림;;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>5.0</td>\n",
       "      <td>음식이 가격대비 양도 많고 맛있는 곳입니다!!!분위기랑 직원들의 반응이 중요하신 분...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>1.0</td>\n",
       "      <td>여기 알바들이 설쳐대는게 아니라면 방송 탔었다고 이리 리뷰가 많은것인가요? 진심 평...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>2.0</td>\n",
       "      <td>음. 웨이팅 십오분했는데 차게 식은 치즈가스가 나왔다. 왕혼가츠도 그냥 쏘쏘.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>5.0</td>\n",
       "      <td>가격은 양에비해서 싸거나 적당하다고 봅니다. 기본 생돈까스는 정말 양이 많슴니다.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>4.0</td>\n",
       "      <td>20160515</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_rating                                            comment  label\n",
       "0             5.0                                         ❤️❤️❤️❤️❤️    1.0\n",
       "1             5.0                                           오래 해주세요🙏    1.0\n",
       "2             5.0       와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!    1.0\n",
       "3             5.0  오늘의 메뉴 너무 좋아요. 어쩜 갖가지 닭요리를 그렇게 잘하시죠?!! 이런 리뷰 잘...    1.0\n",
       "4             1.0                                  여기서 밥먹고 둘다 장염걸림;;    0.0\n",
       "...           ...                                                ...    ...\n",
       "2196          5.0  음식이 가격대비 양도 많고 맛있는 곳입니다!!!분위기랑 직원들의 반응이 중요하신 분...    1.0\n",
       "2197          1.0  여기 알바들이 설쳐대는게 아니라면 방송 탔었다고 이리 리뷰가 많은것인가요? 진심 평...    0.0\n",
       "2198          2.0        음. 웨이팅 십오분했는데 차게 식은 치즈가스가 나왔다. 왕혼가츠도 그냥 쏘쏘.    0.0\n",
       "2199          5.0      가격은 양에비해서 싸거나 적당하다고 봅니다. 기본 생돈까스는 정말 양이 많슴니다.    1.0\n",
       "2200          4.0                                           20160515    1.0\n",
       "\n",
       "[2201 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hong3=hong[hong['label']==3].index.tolist()\n",
    "hong.drop(hong3,axis=0,inplace=True)\n",
    "hong=hong.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kakao_content</th>\n",
       "      <th>kakao_star</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>차돌쭈꾸미 새로생겨서 먹어봤는디 정말맛있어요존맛탱구리구리 오늘도 아주 잘먹구갑니다 ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>진짜 맛집 근처 다른 해물찜들 먹다가 가게가 보여서 들어와 봤는데 진짜 맛있네요벌써...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>처음 방문했는데 생각보다 많이 달라서 당황했습니다 천안본점에는 밑반찬만 6가지 정도...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>지금껏 먹어본 해물찜과는 정말 다르게 너무 맛있어서 먹고 포장까지 해왔습니다새우찜도...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>생고기 맛 좋아서 추천합니다</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>음식먹고 기분나쁘기는 처음이네요 일단 사장인지 종업원인지 서비스마인드 아주 저급하네...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>여기 너무맛있음 1인분에 200g씩 주고 찌개도 듬뿍임 ㅋㅋ신선하게 정육점에서 바로...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>환기만 잘 되면완벽한 삼겹살집</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>고기도 괜찮고 특히 쌈야채 듬뿍 주셔서 넘 좋네요근데 왜 평점테러를 한 거지</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>맛있음</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         kakao_content  kakao_star  label\n",
       "0    차돌쭈꾸미 새로생겨서 먹어봤는디 정말맛있어요존맛탱구리구리 오늘도 아주 잘먹구갑니다 ...         5.0    1.0\n",
       "1    진짜 맛집 근처 다른 해물찜들 먹다가 가게가 보여서 들어와 봤는데 진짜 맛있네요벌써...         5.0    1.0\n",
       "2    처음 방문했는데 생각보다 많이 달라서 당황했습니다 천안본점에는 밑반찬만 6가지 정도...         1.0    0.0\n",
       "3    지금껏 먹어본 해물찜과는 정말 다르게 너무 맛있어서 먹고 포장까지 해왔습니다새우찜도...         5.0    1.0\n",
       "4                                      생고기 맛 좋아서 추천합니다         5.0    1.0\n",
       "..                                                 ...         ...    ...\n",
       "960  음식먹고 기분나쁘기는 처음이네요 일단 사장인지 종업원인지 서비스마인드 아주 저급하네...         1.0    0.0\n",
       "961  여기 너무맛있음 1인분에 200g씩 주고 찌개도 듬뿍임 ㅋㅋ신선하게 정육점에서 바로...         5.0    1.0\n",
       "962                                   환기만 잘 되면완벽한 삼겹살집         5.0    1.0\n",
       "963         고기도 괜찮고 특히 쌈야채 듬뿍 주셔서 넘 좋네요근데 왜 평점테러를 한 거지         5.0    1.0\n",
       "964                                                맛있음         5.0    1.0\n",
       "\n",
       "[965 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kakao3=kakao[kakao['label']==3].index.tolist()\n",
    "kakao.drop(kakao3,axis=0,inplace=True)\n",
    "kakao=kakao.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrs=rr[rr['label']==3.5].index.tolist()\n",
    "rr.drop(rrs,axis=0,inplace=True)\n",
    "rr=rr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hong=hong.drop('user_rating',axis=1)\n",
    "kakao=kakao.drop('kakao_star',axis=1)\n",
    "rr=rr.drop('4',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라벨명 바꾸기\n",
    "hong.rename(columns={'comment':'review'},inplace=True)\n",
    "kakao.rename(columns={'kakao_content':'review'},inplace=True)\n",
    "rr.rename(columns={'맛있는데 고기 양이 조금 적을 수도.':'review'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>❤️❤️❤️❤️❤️</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오래 해주세요🙏</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>오늘의 메뉴 너무 좋아요. 어쩜 갖가지 닭요리를 그렇게 잘하시죠?!! 이런 리뷰 잘...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>여기서 밥먹고 둘다 장염걸림;;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0                                         ❤️❤️❤️❤️❤️    1.0\n",
       "1                                           오래 해주세요🙏    1.0\n",
       "2       와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!    1.0\n",
       "3  오늘의 메뉴 너무 좋아요. 어쩜 갖가지 닭요리를 그렇게 잘하시죠?!! 이런 리뷰 잘...    1.0\n",
       "4                                  여기서 밥먹고 둘다 장염걸림;;    0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([hong,kakao,food_review,thoure,rr],axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14227 entries, 0 to 14226\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   review  14227 non-null  object \n",
      " 1   label   14227 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 222.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('리뷰_pre_trained.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-addons) (23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\KoBERT 감성분류\\리뷰_pre_trained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14227 entries, 0 to 14226\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   review  14227 non-null  object \n",
      " 1   label   14227 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 222.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    12279\n",
       "0.0     1948\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14227, 2)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.863077\n",
       "0.0    0.136923\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟레이블 분포 확인 타겟레이블 분포 확인\n",
    "df['label'].value_counts(normalize=True)\n",
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13515, 2) (712, 2)\n"
     ]
    }
   ],
   "source": [
    "#훈련데이터와 검증데이터로 분리\n",
    "\n",
    "train,test=train_test_split(df,\n",
    "                            random_state=0,\n",
    "                            test_size=0.05,\n",
    "                            stratify=df['label'])\n",
    "\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.reset_index(drop=True)\n",
    "test=test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11664\n",
       "0     1851\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    615\n",
       "0     97\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
    "                     \"vocab_txt\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = u'▁'\n",
    "\n",
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "        SentencePiece based tokenizer. Peculiarities:\n",
    "            - requires `SentencePiece `_\n",
    "    \"\"\"\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_file,\n",
    "            vocab_txt,\n",
    "            do_lower_case=False,\n",
    "            remove_space=True,\n",
    "            keep_accents=False,\n",
    "            unk_token=\"[UNK]\",\n",
    "            sep_token=\"[SEP]\",\n",
    "            pad_token=\"[PAD]\",\n",
    "            cls_token=\"[CLS]\",\n",
    "            mask_token=\"[MASK]\",\n",
    "            **kwargs):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        self.max_len_single_sentence = self.model_max_length - 2  # take into account special tokens\n",
    "        self.max_len_sentences_pair = self.model_max_length - 3  # take into account special tokens\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize('NFKD', outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
    "        \"\"\" Tokenize a string. \"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        if not sample:\n",
    "            pieces = self.sp_model.EncodeAsPieces(text)\n",
    "        else:\n",
    "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index, return_unicode=True):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A RoBERTa sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A BERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "            to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer_78b3253a26.model from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\tokenizer_78b3253a26.model\n",
      "loading file vocab.txt from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"monologg/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "#kobert 토크나이즈 임포트\n",
    "\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 517, 6983, 3298, 5585, 3466, 3742, 6821, 6827, 7275, 5, 1458, 6164, 7141, 5439, 2207, 7395, 4928, 7792, 1562, 1967, 7141, 5703, 5, 5, 3924, 781, 5400, 6999, 5, 3]\n",
      "['▁', '와', '▁여', '길', '▁왜', '▁이제', '알', '았', '죠', '!', '▁너무', '맛', '있', '고', '▁반', '찬', '▁하나', '하나', '▁다', '▁맛', '있', '네요', '!', '!', '▁자주', '▁갈', '게', '요', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!\t\"))\n",
    "print(tokenizer.tokenize(\"와 여길 왜 이제알았죠! 너무맛있고 반찬 하나하나 다 맛있네요!! 자주 갈게요!\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13515 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 13515/13515 [00:04<00:00, 3198.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "    \n",
    "    SEQ_LEN = 64 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
    "    \n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        # token : 문장을 토큰화함\n",
    "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "       \n",
    "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "        \n",
    "        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        segments.append(segment)\n",
    "        \n",
    "        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n",
    "        targets.append(data_df[LABEL_COLUMN][i])\n",
    "\n",
    "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정    \n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return [tokens, masks, segments], targets\n",
    "\n",
    "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
    "def load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "    return data_x, data_y\n",
    "\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "# 긍부정 문장을 포함하고 있는 칼럼\n",
    "DATA_COLUMN = \"review\"\n",
    "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "# train 데이터를 버트 인풋에 맞게 변환\n",
    "train_x, train_y = load_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 712/712 [00:00<00:00, 2861.42it/s]\n"
     ]
    }
   ],
   "source": [
    "test_x,test_y=load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y = tf.expand_dims(train_y, axis=-1)\n",
    "#test_y = tf.expand_dims(test_y, axis=-1)\n",
    "train_y = np.expand_dims(train_y, axis=1)\n",
    "test_y = np.expand_dims(test_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13515,)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoBERT 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\pytorch_model.bin\n",
      "Loading PyTorch weights from C:\\Users\\user\\.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\pytorch_model.bin\n",
      "PyTorch checkpoint contains 92,186,880 parameters\n",
      "Loaded 92,186,880 parameters in the TF 2.0 model.\n",
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
    "\n",
    "# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
    "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
    "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
    "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
    "\n",
    "# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
    "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs=bert_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model_2')>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 768])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\tensorflow_addons\\optimizers\\rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Rectified Adam 옵티마이저 사용\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# 총 batch size * 4 epoch = 2344 * 4\n",
    "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n",
    "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n",
    "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
    "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_masks (InputLayer)       [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_segment (InputLayer)     [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  92186880    ['input_word_ids[0][0]',         \n",
      "                                thPoolingAndCrossAt               'input_masks[0][0]',            \n",
      "                                tentions(last_hidde               'input_segment[0][0]']          \n",
      "                                n_state=(None, 64,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_120 (Dropout)          (None, 768)          0           ['tf_bert_model_2[0][1]']        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            769         ['dropout_120[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 92,187,649\n",
      "Trainable params: 92,187,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "212/212 [==============================] - 2490s 12s/step - loss: 0.2269 - accuracy: 0.9217 - val_loss: 0.2376 - val_accuracy: 0.9199\n",
      "Epoch 2/2\n",
      "212/212 [==============================] - 2379s 11s/step - loss: 0.1902 - accuracy: 0.9383 - val_loss: 0.2665 - val_accuracy: 0.9143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ea45279d0>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.fit(train_x,train_y,epochs=2,shuffle=True,batch_size=64,validation_data=(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_model.save('KoBERT_model')\n",
    "#sentiment_model.save_weights('saveweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_convert_data(data_df):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "\n",
    "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        tokens.append(token)\n",
    "        segments.append(segment)\n",
    "        masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
    "def predict_load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_x = predict_convert_data(data_df)\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/712 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 712/712 [00:00<00:00, 2728.20it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = predict_load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 42s 2s/step\n"
     ]
    }
   ],
   "source": [
    "preds = sentiment_model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 1)\n",
      "(712,)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1\n",
       "6      1\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     1\n",
       "13     0\n",
       "14     1\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18     1\n",
       "19     0\n",
       "20     1\n",
       "21     1\n",
       "22     1\n",
       "23     1\n",
       "24     1\n",
       "25     1\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     0\n",
       "30     1\n",
       "31     0\n",
       "32     1\n",
       "33     1\n",
       "34     1\n",
       "35     1\n",
       "36     0\n",
       "37     0\n",
       "38     0\n",
       "39     1\n",
       "40     1\n",
       "41     1\n",
       "42     1\n",
       "43     1\n",
       "44     1\n",
       "45     1\n",
       "46     1\n",
       "47     1\n",
       "48     1\n",
       "49     1\n",
       "50     1\n",
       "51     0\n",
       "52     1\n",
       "53     1\n",
       "54     0\n",
       "55     0\n",
       "56     1\n",
       "57     1\n",
       "58     1\n",
       "59     1\n",
       "60     1\n",
       "61     1\n",
       "62     1\n",
       "63     1\n",
       "64     0\n",
       "65     1\n",
       "66     1\n",
       "67     1\n",
       "68     0\n",
       "69     1\n",
       "70     1\n",
       "71     1\n",
       "72     1\n",
       "73     1\n",
       "74     1\n",
       "75     1\n",
       "76     1\n",
       "77     1\n",
       "78     1\n",
       "79     1\n",
       "80     1\n",
       "81     0\n",
       "82     1\n",
       "83     1\n",
       "84     1\n",
       "85     1\n",
       "86     1\n",
       "87     1\n",
       "88     1\n",
       "89     1\n",
       "90     1\n",
       "91     1\n",
       "92     1\n",
       "93     1\n",
       "94     1\n",
       "95     1\n",
       "96     1\n",
       "97     1\n",
       "98     0\n",
       "99     1\n",
       "100    1\n",
       "101    1\n",
       "102    1\n",
       "103    0\n",
       "104    0\n",
       "105    1\n",
       "106    1\n",
       "107    1\n",
       "108    1\n",
       "109    1\n",
       "110    1\n",
       "111    1\n",
       "112    1\n",
       "113    1\n",
       "114    1\n",
       "115    1\n",
       "116    0\n",
       "117    1\n",
       "118    1\n",
       "119    1\n",
       "120    1\n",
       "121    1\n",
       "122    1\n",
       "123    1\n",
       "124    1\n",
       "125    1\n",
       "126    1\n",
       "127    1\n",
       "128    0\n",
       "129    0\n",
       "130    0\n",
       "131    1\n",
       "132    1\n",
       "133    1\n",
       "134    1\n",
       "135    1\n",
       "136    1\n",
       "137    1\n",
       "138    1\n",
       "139    1\n",
       "140    1\n",
       "141    1\n",
       "142    1\n",
       "143    1\n",
       "144    1\n",
       "145    1\n",
       "146    1\n",
       "147    0\n",
       "148    1\n",
       "149    0\n",
       "150    1\n",
       "151    1\n",
       "152    1\n",
       "153    1\n",
       "154    0\n",
       "155    1\n",
       "156    1\n",
       "157    1\n",
       "158    1\n",
       "159    1\n",
       "160    1\n",
       "161    1\n",
       "162    1\n",
       "163    1\n",
       "164    1\n",
       "165    1\n",
       "166    1\n",
       "167    1\n",
       "168    1\n",
       "169    0\n",
       "170    1\n",
       "171    1\n",
       "172    1\n",
       "173    1\n",
       "174    1\n",
       "175    1\n",
       "176    1\n",
       "177    1\n",
       "178    1\n",
       "179    1\n",
       "180    1\n",
       "181    1\n",
       "182    1\n",
       "183    1\n",
       "184    1\n",
       "185    0\n",
       "186    1\n",
       "187    1\n",
       "188    1\n",
       "189    1\n",
       "190    1\n",
       "191    1\n",
       "192    1\n",
       "193    1\n",
       "194    1\n",
       "195    1\n",
       "196    1\n",
       "197    1\n",
       "198    1\n",
       "199    1\n",
       "200    0\n",
       "201    1\n",
       "202    1\n",
       "203    1\n",
       "204    1\n",
       "205    0\n",
       "206    1\n",
       "207    1\n",
       "208    1\n",
       "209    1\n",
       "210    1\n",
       "211    1\n",
       "212    1\n",
       "213    1\n",
       "214    1\n",
       "215    0\n",
       "216    1\n",
       "217    1\n",
       "218    1\n",
       "219    1\n",
       "220    1\n",
       "221    1\n",
       "222    0\n",
       "223    1\n",
       "224    1\n",
       "225    1\n",
       "226    1\n",
       "227    1\n",
       "228    1\n",
       "229    1\n",
       "230    1\n",
       "231    1\n",
       "232    1\n",
       "233    1\n",
       "234    1\n",
       "235    1\n",
       "236    1\n",
       "237    1\n",
       "238    1\n",
       "239    1\n",
       "240    1\n",
       "241    1\n",
       "242    0\n",
       "243    1\n",
       "244    1\n",
       "245    0\n",
       "246    1\n",
       "247    1\n",
       "248    1\n",
       "249    1\n",
       "250    1\n",
       "251    1\n",
       "252    1\n",
       "253    1\n",
       "254    1\n",
       "255    1\n",
       "256    1\n",
       "257    1\n",
       "258    1\n",
       "259    1\n",
       "260    1\n",
       "261    1\n",
       "262    1\n",
       "263    1\n",
       "264    1\n",
       "265    1\n",
       "266    1\n",
       "267    1\n",
       "268    1\n",
       "269    1\n",
       "270    1\n",
       "271    1\n",
       "272    1\n",
       "273    1\n",
       "274    1\n",
       "275    1\n",
       "276    1\n",
       "277    1\n",
       "278    1\n",
       "279    1\n",
       "280    1\n",
       "281    0\n",
       "282    1\n",
       "283    1\n",
       "284    0\n",
       "285    1\n",
       "286    1\n",
       "287    1\n",
       "288    1\n",
       "289    1\n",
       "290    1\n",
       "291    1\n",
       "292    1\n",
       "293    1\n",
       "294    1\n",
       "295    1\n",
       "296    1\n",
       "297    1\n",
       "298    1\n",
       "299    1\n",
       "300    1\n",
       "301    1\n",
       "302    1\n",
       "303    1\n",
       "304    1\n",
       "305    1\n",
       "306    1\n",
       "307    0\n",
       "308    1\n",
       "309    1\n",
       "310    1\n",
       "311    0\n",
       "312    0\n",
       "313    1\n",
       "314    0\n",
       "315    1\n",
       "316    1\n",
       "317    1\n",
       "318    0\n",
       "319    0\n",
       "320    1\n",
       "321    1\n",
       "322    0\n",
       "323    1\n",
       "324    1\n",
       "325    1\n",
       "326    1\n",
       "327    1\n",
       "328    1\n",
       "329    0\n",
       "330    1\n",
       "331    1\n",
       "332    1\n",
       "333    1\n",
       "334    1\n",
       "335    1\n",
       "336    1\n",
       "337    1\n",
       "338    1\n",
       "339    0\n",
       "340    0\n",
       "341    1\n",
       "342    1\n",
       "343    0\n",
       "344    0\n",
       "345    1\n",
       "346    1\n",
       "347    1\n",
       "348    1\n",
       "349    1\n",
       "350    1\n",
       "351    1\n",
       "352    1\n",
       "353    1\n",
       "354    1\n",
       "355    1\n",
       "356    1\n",
       "357    1\n",
       "358    1\n",
       "359    1\n",
       "360    1\n",
       "361    1\n",
       "362    0\n",
       "363    1\n",
       "364    1\n",
       "365    1\n",
       "366    1\n",
       "367    1\n",
       "368    1\n",
       "369    1\n",
       "370    1\n",
       "371    1\n",
       "372    1\n",
       "373    1\n",
       "374    1\n",
       "375    1\n",
       "376    1\n",
       "377    1\n",
       "378    1\n",
       "379    1\n",
       "380    1\n",
       "381    1\n",
       "382    1\n",
       "383    1\n",
       "384    1\n",
       "385    1\n",
       "386    1\n",
       "387    1\n",
       "388    1\n",
       "389    1\n",
       "390    1\n",
       "391    1\n",
       "392    1\n",
       "393    1\n",
       "394    1\n",
       "395    1\n",
       "396    1\n",
       "397    1\n",
       "398    1\n",
       "399    0\n",
       "400    1\n",
       "401    1\n",
       "402    1\n",
       "403    1\n",
       "404    1\n",
       "405    1\n",
       "406    1\n",
       "407    1\n",
       "408    1\n",
       "409    1\n",
       "410    1\n",
       "411    1\n",
       "412    1\n",
       "413    1\n",
       "414    1\n",
       "415    1\n",
       "416    1\n",
       "417    1\n",
       "418    0\n",
       "419    1\n",
       "420    1\n",
       "421    0\n",
       "422    1\n",
       "423    1\n",
       "424    1\n",
       "425    1\n",
       "426    1\n",
       "427    1\n",
       "428    1\n",
       "429    1\n",
       "430    1\n",
       "431    1\n",
       "432    1\n",
       "433    1\n",
       "434    0\n",
       "435    1\n",
       "436    1\n",
       "437    1\n",
       "438    1\n",
       "439    0\n",
       "440    1\n",
       "441    1\n",
       "442    1\n",
       "443    0\n",
       "444    1\n",
       "445    1\n",
       "446    1\n",
       "447    0\n",
       "448    1\n",
       "449    1\n",
       "450    1\n",
       "451    1\n",
       "452    1\n",
       "453    1\n",
       "454    1\n",
       "455    1\n",
       "456    1\n",
       "457    0\n",
       "458    1\n",
       "459    1\n",
       "460    1\n",
       "461    1\n",
       "462    1\n",
       "463    1\n",
       "464    1\n",
       "465    0\n",
       "466    1\n",
       "467    1\n",
       "468    1\n",
       "469    1\n",
       "470    1\n",
       "471    1\n",
       "472    1\n",
       "473    1\n",
       "474    1\n",
       "475    1\n",
       "476    1\n",
       "477    1\n",
       "478    1\n",
       "479    1\n",
       "480    1\n",
       "481    1\n",
       "482    1\n",
       "483    1\n",
       "484    1\n",
       "485    1\n",
       "486    1\n",
       "487    1\n",
       "488    1\n",
       "489    0\n",
       "490    1\n",
       "491    1\n",
       "492    1\n",
       "493    1\n",
       "494    1\n",
       "495    1\n",
       "496    1\n",
       "497    0\n",
       "498    1\n",
       "499    1\n",
       "500    0\n",
       "501    1\n",
       "502    1\n",
       "503    1\n",
       "504    0\n",
       "505    0\n",
       "506    1\n",
       "507    1\n",
       "508    0\n",
       "509    1\n",
       "510    1\n",
       "511    1\n",
       "512    1\n",
       "513    1\n",
       "514    1\n",
       "515    1\n",
       "516    1\n",
       "517    0\n",
       "518    1\n",
       "519    1\n",
       "520    1\n",
       "521    1\n",
       "522    1\n",
       "523    1\n",
       "524    0\n",
       "525    1\n",
       "526    1\n",
       "527    1\n",
       "528    1\n",
       "529    0\n",
       "530    1\n",
       "531    1\n",
       "532    1\n",
       "533    0\n",
       "534    0\n",
       "535    1\n",
       "536    1\n",
       "537    1\n",
       "538    0\n",
       "539    1\n",
       "540    1\n",
       "541    1\n",
       "542    1\n",
       "543    1\n",
       "544    0\n",
       "545    1\n",
       "546    1\n",
       "547    1\n",
       "548    1\n",
       "549    1\n",
       "550    1\n",
       "551    1\n",
       "552    1\n",
       "553    0\n",
       "554    1\n",
       "555    1\n",
       "556    1\n",
       "557    1\n",
       "558    1\n",
       "559    1\n",
       "560    1\n",
       "561    1\n",
       "562    1\n",
       "563    1\n",
       "564    1\n",
       "565    1\n",
       "566    1\n",
       "567    0\n",
       "568    1\n",
       "569    1\n",
       "570    1\n",
       "571    0\n",
       "572    1\n",
       "573    1\n",
       "574    1\n",
       "575    1\n",
       "576    1\n",
       "577    1\n",
       "578    1\n",
       "579    1\n",
       "580    0\n",
       "581    1\n",
       "582    1\n",
       "583    1\n",
       "584    1\n",
       "585    1\n",
       "586    0\n",
       "587    1\n",
       "588    1\n",
       "589    1\n",
       "590    0\n",
       "591    1\n",
       "592    1\n",
       "593    1\n",
       "594    1\n",
       "595    0\n",
       "596    1\n",
       "597    1\n",
       "598    1\n",
       "599    1\n",
       "600    0\n",
       "601    1\n",
       "602    1\n",
       "603    1\n",
       "604    1\n",
       "605    1\n",
       "606    0\n",
       "607    1\n",
       "608    1\n",
       "609    1\n",
       "610    1\n",
       "611    1\n",
       "612    0\n",
       "613    0\n",
       "614    1\n",
       "615    1\n",
       "616    1\n",
       "617    1\n",
       "618    1\n",
       "619    1\n",
       "620    1\n",
       "621    1\n",
       "622    1\n",
       "623    1\n",
       "624    1\n",
       "625    0\n",
       "626    1\n",
       "627    1\n",
       "628    1\n",
       "629    1\n",
       "630    1\n",
       "631    1\n",
       "632    0\n",
       "633    1\n",
       "634    0\n",
       "635    1\n",
       "636    1\n",
       "637    1\n",
       "638    1\n",
       "639    0\n",
       "640    0\n",
       "641    1\n",
       "642    1\n",
       "643    1\n",
       "644    1\n",
       "645    1\n",
       "646    1\n",
       "647    1\n",
       "648    1\n",
       "649    0\n",
       "650    1\n",
       "651    1\n",
       "652    1\n",
       "653    1\n",
       "654    1\n",
       "655    1\n",
       "656    1\n",
       "657    1\n",
       "658    1\n",
       "659    0\n",
       "660    1\n",
       "661    1\n",
       "662    1\n",
       "663    1\n",
       "664    1\n",
       "665    1\n",
       "666    1\n",
       "667    0\n",
       "668    1\n",
       "669    1\n",
       "670    1\n",
       "671    1\n",
       "672    1\n",
       "673    1\n",
       "674    1\n",
       "675    1\n",
       "676    1\n",
       "677    1\n",
       "678    0\n",
       "679    1\n",
       "680    1\n",
       "681    1\n",
       "682    1\n",
       "683    1\n",
       "684    0\n",
       "685    0\n",
       "686    1\n",
       "687    1\n",
       "688    1\n",
       "689    1\n",
       "690    1\n",
       "691    1\n",
       "692    1\n",
       "693    1\n",
       "694    1\n",
       "695    0\n",
       "696    1\n",
       "697    1\n",
       "698    0\n",
       "699    1\n",
       "700    0\n",
       "701    0\n",
       "702    1\n",
       "703    1\n",
       "704    1\n",
       "705    0\n",
       "706    0\n",
       "707    0\n",
       "708    1\n",
       "709    1\n",
       "710    1\n",
       "711    1\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.49      0.61        97\n",
      "           1       0.92      0.98      0.95       615\n",
      "\n",
      "    accuracy                           0.91       712\n",
      "   macro avg       0.86      0.74      0.78       712\n",
      "weighted avg       0.91      0.91      0.91       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = test['label']\n",
    "# F1 Score 확인\n",
    "print(classification_report(y_true, np.round(preds,0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 데이터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    token = tokenizer.encode(data, max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "    \n",
    "    num_zeros = token.count(0) \n",
    "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros \n",
    "    segment = [0]*SEQ_LEN\n",
    "\n",
    "    tokens.append(token)\n",
    "    segments.append(segment)\n",
    "    masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "def review_evaluation_predict(sentence):\n",
    "    data_x = sentence_convert_data(sentence)\n",
    "    predict = sentiment_model.predict(data_x)\n",
    "    predict_value = np.ravel(predict)\n",
    "    predict_answer = np.round(predict_value,0).item()\n",
    "    \n",
    "    if predict_answer == 0:\n",
    "      print(\"(부정 확률 : %.2f) 부정적인 리뷰입니다.\" % (1-predict_value))\n",
    "    elif predict_answer == 1:\n",
    "      print(\"(긍정 확률 : %.2f) 긍정적인 리뷰입니다.\" % predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 220ms/step\n",
      "(부정 확률 : 0.93) 부정적인 리뷰입니다.\n"
     ]
    }
   ],
   "source": [
    "review_evaluation_predict(\"다시는 가고 싶지 않아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 194ms/step\n",
      "(긍정 확률 : 0.98) 긍정적인 리뷰입니다.\n"
     ]
    }
   ],
   "source": [
    "review_evaluation_predict(\"맛은 없는데 식당 내부 분위기는 좋아요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 211ms/step\n",
      "(긍정 확률 : 0.98) 긍정적인 리뷰입니다.\n"
     ]
    }
   ],
   "source": [
    "review_evaluation_predict(\"맛있어요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    token = tokenizer.encode(data, max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "    \n",
    "    num_zeros = token.count(0) \n",
    "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros \n",
    "    segment = [0]*SEQ_LEN\n",
    "\n",
    "    tokens.append(token)\n",
    "    segments.append(segment)\n",
    "    masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "def review_evaluation_predict(sentence):\n",
    "    data_x = sentence_convert_data(sentence)\n",
    "    predict = sentiment_model.predict(data_x)\n",
    "    predict_value = np.ravel(predict)\n",
    "    predict_answer = np.round(predict_value,0).item()\n",
    "    \n",
    "    if predict_answer == 0:\n",
    "      return \"(부정 확률 : %.2f) 부정\" % (1-predict_value)\n",
    "    elif predict_answer == 1:\n",
    "      return \"(긍정 확률 : %.2f) 긍정\" % predict_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>상가업소번호</th>\n",
       "      <th>가게명</th>\n",
       "      <th>원래이름</th>\n",
       "      <th>리뷰</th>\n",
       "      <th>분리리뷰</th>\n",
       "      <th>리뷰날짜</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>칼만두국을 먹었는데 사골베이스에 조금 슴슴한 편인데 맛이 꽤 훌륭하다.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>면발은 하늘하늘 부드러운 편이다.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>테이블 간격이 좁고 서버 이모님들이 살짝 불친절한 편이라 온전히 유명세의 맛경험을 ...</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>주말에 방문하면 웨이팅이 깁니다. 한시간정도 대기하면 드실 수 있어요. 만두가 깔끔...</td>\n",
       "      <td>주말에 방문하면 웨이팅이 깁니다.</td>\n",
       "      <td>2023-2-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     상가업소번호   가게명  원래이름                                                 리뷰  \\\n",
       "0  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "1  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "2  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "3  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "4  20592276  깡통만두  깡통만두  주말에 방문하면 웨이팅이 깁니다. 한시간정도 대기하면 드실 수 있어요. 만두가 깔끔...   \n",
       "\n",
       "                                                분리리뷰       리뷰날짜  \n",
       "0                       만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.   2023-3-9  \n",
       "1            칼만두국을 먹었는데 사골베이스에 조금 슴슴한 편인데 맛이 꽤 훌륭하다.   2023-3-9  \n",
       "2                                 면발은 하늘하늘 부드러운 편이다.   2023-3-9  \n",
       "3  테이블 간격이 좁고 서버 이모님들이 살짝 불친절한 편이라 온전히 유명세의 맛경험을 ...   2023-3-9  \n",
       "4                                 주말에 방문하면 웨이팅이 깁니다.  2023-2-19  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\data_git\\data\\diningcode_data\\다이닝코드 북촌 리뷰 분리.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n"
     ]
    }
   ],
   "source": [
    "data['evaluation'] = data['분리리뷰'].apply(review_evaluation_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('test_review_predict.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('test_review_predict.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
