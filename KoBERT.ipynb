{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Set, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed ê³ ì •\n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED']=str(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_review=pd.read_csv(r'C:\\Users\\user\\OneDrive\\ë°”íƒ•í™”~1-DESKTOP-R00ORLS-348\\ìº¡ìŠ¤í†¤\\KoBERT ê°ì„±ë¶„ë¥˜\\data\\food_review.csv')\n",
    "kakao=pd.read_csv(r'C:\\Users\\user\\OneDrive\\ë°”íƒ•í™”~1-DESKTOP-R00ORLS-348\\ìº¡ìŠ¤í†¤\\KoBERT ê°ì„±ë¶„ë¥˜\\data\\kakao_review_0to500.csv',encoding='cp949')\n",
    "thoure=pd.read_csv(r'C:\\Users\\user\\OneDrive\\ë°”íƒ•í™”~1-DESKTOP-R00ORLS-348\\ìº¡ìŠ¤í†¤\\KoBERT ê°ì„±ë¶„ë¥˜\\data\\1500ê°œ_ë¦¬ë·°_label.CSV',encoding='cp949')\n",
    "hong=pd.read_csv(r'C:\\Users\\user\\OneDrive\\ë°”íƒ•í™”~1-DESKTOP-R00ORLS-348\\ìº¡ìŠ¤í†¤\\KoBERT ê°ì„±ë¶„ë¥˜\\data\\í™ëŒ€_ë§›ì§‘_ë¦¬ë·°_ë°ì´í„°_ver5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>ë§›ìˆëŠ”ë° ê³ ê¸° ì–‘ì´ ì¡°ê¸ˆ ì ì„ ìˆ˜ë„.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>ì ì‹¬ì‹œê°„ë§Œë˜ë©´ ë¬¸ì „ì„±ì‹œì¸ê³³. í˜¼ìì™€ì„œ ë¨¹ëŠ”ì‚¬ëŒë„ ë§ìŒ. ì§„ì§œ ê¼¬ìˆ˜ìš´ ì²­êµ­ì¥ ìƒê°ë‚ ë•Œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>ë§›ìˆì–´ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>ë°˜ì°¬ì´ ë¶€ì‹¤í•˜ë„¤ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>ë§›ìˆì–´ìš”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     4                               ë§›ìˆëŠ”ë° ê³ ê¸° ì–‘ì´ ì¡°ê¸ˆ ì ì„ ìˆ˜ë„.\n",
       "0  4.0  ì ì‹¬ì‹œê°„ë§Œë˜ë©´ ë¬¸ì „ì„±ì‹œì¸ê³³. í˜¼ìì™€ì„œ ë¨¹ëŠ”ì‚¬ëŒë„ ë§ìŒ. ì§„ì§œ ê¼¬ìˆ˜ìš´ ì²­êµ­ì¥ ìƒê°ë‚ ë•Œ...\n",
       "1  4.5                                               ë§›ìˆì–´ìš”\n",
       "2  5.0                                                  .\n",
       "3  4.5                                          ë°˜ì°¬ì´ ë¶€ì‹¤í•˜ë„¤ìš”\n",
       "4  4.0                                               ë§›ìˆì–´ìš”"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr=pd.read_csv(r'C:\\Users\\user\\OneDrive\\ë°”íƒ•í™”~1-DESKTOP-R00ORLS-348\\ìº¡ìŠ¤í†¤\\KoBERT ê°ì„±ë¶„ë¥˜\\data\\restaurant_review.txt',sep='\\t')\n",
    "rr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_review.drop('score',axis=1,inplace=True)\n",
    "kakao.drop(['store_id','kakao_nickname','kakao_date'],axis=1,inplace=True)\n",
    "thoure=thoure[['review','label']]\n",
    "hong=hong[['user_rating','comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì¹œì ˆí•˜ì‹œê³  ê¹”ë”í•˜ê³  ì¢‹ì•˜ìŠµë‹ˆë‹¤</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì¡°ìš©í•˜ê³  ê³ ê¸°ë„ êµ¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê°ˆë¹„íƒ•ê³¼ ëƒ‰ë©´, ìœ¡íšŒë¹„ë¹”ë°¥ì´ ë§›ìˆìŠµë‹ˆë‹¤.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ëŒ€ì²´ì ìœ¼ë¡œ ë§Œì¡±í•˜ë‚˜\\nì™€ì¸ì˜ êµ¬ì„±ì´ ì‚´ì§ ì•„ì‰¬ì›€</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ê³ ê¸°ë„ ë§›ìˆê³  ì„œë¹„ìŠ¤ëŠ” ë” ìµœê³ ì…ë‹ˆë‹¤~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       review  y\n",
       "0            ì¹œì ˆí•˜ì‹œê³  ê¹”ë”í•˜ê³  ì¢‹ì•˜ìŠµë‹ˆë‹¤  1\n",
       "1                  ì¡°ìš©í•˜ê³  ê³ ê¸°ë„ êµ¿  1\n",
       "2      ê°ˆë¹„íƒ•ê³¼ ëƒ‰ë©´, ìœ¡íšŒë¹„ë¹”ë°¥ì´ ë§›ìˆìŠµë‹ˆë‹¤.  1\n",
       "3  ëŒ€ì²´ì ìœ¼ë¡œ ë§Œì¡±í•˜ë‚˜\\nì™€ì¸ì˜ êµ¬ì„±ì´ ì‚´ì§ ì•„ì‰¬ì›€  1\n",
       "4       ê³ ê¸°ë„ ë§›ìˆê³  ì„œë¹„ìŠ¤ëŠ” ë” ìµœê³ ì…ë‹ˆë‹¤~  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kakao_content</th>\n",
       "      <th>kakao_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì°¨ëŒì­ˆê¾¸ë¯¸ ìƒˆë¡œìƒê²¨ì„œ ë¨¹ì–´ë´¤ëŠ”ë”” ì •ë§ë§›ìˆì–´ìš”ì¡´ë§›íƒ±êµ¬ë¦¬êµ¬ë¦¬ ì˜¤ëŠ˜ë„ ì•„ì£¼ ì˜ë¨¹êµ¬ê°‘ë‹ˆë‹¤ ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì§„ì§œ ë§›ì§‘ ê·¼ì²˜ ë‹¤ë¥¸ í•´ë¬¼ì°œë“¤ ë¨¹ë‹¤ê°€ ê°€ê²Œê°€ ë³´ì—¬ì„œ ë“¤ì–´ì™€ ë´¤ëŠ”ë° ì§„ì§œ ë§›ìˆë„¤ìš”ë²Œì¨...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì²˜ìŒ ë°©ë¬¸í–ˆëŠ”ë° ìƒê°ë³´ë‹¤ ë§ì´ ë‹¬ë¼ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤ ì²œì•ˆë³¸ì ì—ëŠ” ë°‘ë°˜ì°¬ë§Œ 6ê°€ì§€ ì •ë„...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì§€ê¸ˆê» ë¨¹ì–´ë³¸ í•´ë¬¼ì°œê³¼ëŠ” ì •ë§ ë‹¤ë¥´ê²Œ ë„ˆë¬´ ë§›ìˆì–´ì„œ ë¨¹ê³  í¬ì¥ê¹Œì§€ í•´ì™”ìŠµë‹ˆë‹¤ìƒˆìš°ì°œë„...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ìƒê³ ê¸° ë§› ì¢‹ì•„ì„œ ì¶”ì²œí•©ë‹ˆë‹¤</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       kakao_content  kakao_star\n",
       "0  ì°¨ëŒì­ˆê¾¸ë¯¸ ìƒˆë¡œìƒê²¨ì„œ ë¨¹ì–´ë´¤ëŠ”ë”” ì •ë§ë§›ìˆì–´ìš”ì¡´ë§›íƒ±êµ¬ë¦¬êµ¬ë¦¬ ì˜¤ëŠ˜ë„ ì•„ì£¼ ì˜ë¨¹êµ¬ê°‘ë‹ˆë‹¤ ...         5.0\n",
       "1  ì§„ì§œ ë§›ì§‘ ê·¼ì²˜ ë‹¤ë¥¸ í•´ë¬¼ì°œë“¤ ë¨¹ë‹¤ê°€ ê°€ê²Œê°€ ë³´ì—¬ì„œ ë“¤ì–´ì™€ ë´¤ëŠ”ë° ì§„ì§œ ë§›ìˆë„¤ìš”ë²Œì¨...         5.0\n",
       "2  ì²˜ìŒ ë°©ë¬¸í–ˆëŠ”ë° ìƒê°ë³´ë‹¤ ë§ì´ ë‹¬ë¼ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤ ì²œì•ˆë³¸ì ì—ëŠ” ë°‘ë°˜ì°¬ë§Œ 6ê°€ì§€ ì •ë„...         1.0\n",
       "3  ì§€ê¸ˆê» ë¨¹ì–´ë³¸ í•´ë¬¼ì°œê³¼ëŠ” ì •ë§ ë‹¤ë¥´ê²Œ ë„ˆë¬´ ë§›ìˆì–´ì„œ ë¨¹ê³  í¬ì¥ê¹Œì§€ í•´ì™”ìŠµë‹ˆë‹¤ìƒˆìš°ì°œë„...         5.0\n",
       "4                                    ìƒê³ ê¸° ë§› ì¢‹ì•„ì„œ ì¶”ì²œí•©ë‹ˆë‹¤         5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì£¼ì°¨ì‹œì„¤ì¢‹ê³  ë„“ê³  ê¹”ë”í•˜ê³  ì¢‹ì•„ìš”. ì§‘ ê·¼ì²˜ë¼ ë”ìš± ì¢‹ë„¤ìš”</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•„í¬ì¹´í†  ë„˜ë„˜ ë§›ìˆì–´ìš”??</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë„“ê³  ì¾Œì í•œ ì‹¤ë‚´ê°€ ì¢‹ì•„ìš”. ì»¤í”¼ë„ êµ¿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë§¤ì¥ì´ ë„“ì–´ì„œ ì¢‹ì•„ìš”!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì¹œì ˆí•˜ì‹œê³  ìŒë£Œë„ ë§›ìˆì–´ìš” :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             review  label\n",
       "0  ì£¼ì°¨ì‹œì„¤ì¢‹ê³  ë„“ê³  ê¹”ë”í•˜ê³  ì¢‹ì•„ìš”. ì§‘ ê·¼ì²˜ë¼ ë”ìš± ì¢‹ë„¤ìš”      1\n",
       "1                    ì•„í¬ì¹´í†  ë„˜ë„˜ ë§›ìˆì–´ìš”??      1\n",
       "2             ë„“ê³  ì¾Œì í•œ ì‹¤ë‚´ê°€ ì¢‹ì•„ìš”. ì»¤í”¼ë„ êµ¿      1\n",
       "3                      ë§¤ì¥ì´ ë„“ì–´ì„œ ì¢‹ì•„ìš”!      1\n",
       "4                 ì¹œì ˆí•˜ì‹œê³  ìŒë£Œë„ ë§›ìˆì–´ìš” :)      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ì˜¤ë˜ í•´ì£¼ì„¸ìš”ğŸ™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>ì›¨ì´íŒ…í•´ì„œ ë¨¹ì„ ë§›ì€ ì•„ë‹ˆì—ìš”â€¦ ë‹­ë³´ìŒˆì€ ë³´í†µì´ì—ˆê³  ë¹„ë¹”êµ­ìˆ˜ë¼ê³  í•´ì„œ ë‹¹ì—°íˆ êµ­ë¬¼ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_rating                                            comment\n",
       "0          5.0                                                NaN\n",
       "1          5.0                                         â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸\n",
       "2          5.0                                           ì˜¤ë˜ í•´ì£¼ì„¸ìš”ğŸ™\n",
       "3          3.0  ì›¨ì´íŒ…í•´ì„œ ë¨¹ì„ ë§›ì€ ì•„ë‹ˆì—ìš”â€¦ ë‹­ë³´ìŒˆì€ ë³´í†µì´ì—ˆê³  ë¹„ë¹”êµ­ìˆ˜ë¼ê³  í•´ì„œ ë‹¹ì—°íˆ êµ­ë¬¼ ...\n",
       "4          5.0       ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(food_review.head())\n",
    "display(kakao.head())\n",
    "display(thoure.head())\n",
    "display(hong.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nullê°’ ì—†ì• ê¸°\n",
    "kakao.dropna(axis=0,inplace=True)\n",
    "hong.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 545 entries, 0 to 544\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  545 non-null    object\n",
      " 1   label   545 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 965 entries, 0 to 964\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   kakao_content  965 non-null    object \n",
      " 1   label          965 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 15.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1574 entries, 0 to 1573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  1574 non-null   object\n",
      " 1   label   1574 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2201 entries, 0 to 2200\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   comment  2201 non-null   object \n",
      " 1   label    2201 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 34.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(food_review.info())\n",
    "display(kakao.info())\n",
    "display(thoure.info())\n",
    "display(hong.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5,4ëŠ” 1ì˜ ê¸ì •ë ˆì´ë¸”, 1,2ëŠ” ë¶€ì •ë ˆì´ë¸”, 3ì€ ì‚­ì œ\n",
    "def label_mapping(labels):\n",
    "    return [1 if label in [5, 4, 4.5] else 0 if label in [1, 2,0.5,2.5,1.5] else label for label in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kakao_new_labels=label_mapping(kakao['kakao_star'])\n",
    "new_labels=label_mapping(hong['user_rating'])\n",
    "rr_labels=label_mapping(rr['4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "kakao['label']=kakao_new_labels\n",
    "hong['label']=new_labels\n",
    "rr['label']=rr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_rating</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ì˜¤ë˜ í•´ì£¼ì„¸ìš”ğŸ™</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ì˜¤ëŠ˜ì˜ ë©”ë‰´ ë„ˆë¬´ ì¢‹ì•„ìš”. ì–´ì©œ ê°–ê°€ì§€ ë‹­ìš”ë¦¬ë¥¼ ê·¸ë ‡ê²Œ ì˜í•˜ì‹œì£ ?!! ì´ëŸ° ë¦¬ë·° ì˜...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ì—¬ê¸°ì„œ ë°¥ë¨¹ê³  ë‘˜ë‹¤ ì¥ì—¼ê±¸ë¦¼;;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ìŒì‹ì´ ê°€ê²©ëŒ€ë¹„ ì–‘ë„ ë§ê³  ë§›ìˆëŠ” ê³³ì…ë‹ˆë‹¤!!!ë¶„ìœ„ê¸°ë‘ ì§ì›ë“¤ì˜ ë°˜ì‘ì´ ì¤‘ìš”í•˜ì‹  ë¶„...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ì—¬ê¸° ì•Œë°”ë“¤ì´ ì„¤ì³ëŒ€ëŠ”ê²Œ ì•„ë‹ˆë¼ë©´ ë°©ì†¡ íƒ”ì—ˆë‹¤ê³  ì´ë¦¬ ë¦¬ë·°ê°€ ë§ì€ê²ƒì¸ê°€ìš”? ì§„ì‹¬ í‰...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>2.0</td>\n",
       "      <td>ìŒ. ì›¨ì´íŒ… ì‹­ì˜¤ë¶„í–ˆëŠ”ë° ì°¨ê²Œ ì‹ì€ ì¹˜ì¦ˆê°€ìŠ¤ê°€ ë‚˜ì™”ë‹¤. ì™•í˜¼ê°€ì¸ ë„ ê·¸ëƒ¥ ì˜ì˜.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ê°€ê²©ì€ ì–‘ì—ë¹„í•´ì„œ ì‹¸ê±°ë‚˜ ì ë‹¹í•˜ë‹¤ê³  ë´…ë‹ˆë‹¤. ê¸°ë³¸ ìƒëˆê¹ŒìŠ¤ëŠ” ì •ë§ ì–‘ì´ ë§ìŠ´ë‹ˆë‹¤.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>4.0</td>\n",
       "      <td>20160515</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2201 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_rating                                            comment  label\n",
       "0             5.0                                         â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸    1.0\n",
       "1             5.0                                           ì˜¤ë˜ í•´ì£¼ì„¸ìš”ğŸ™    1.0\n",
       "2             5.0       ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!    1.0\n",
       "3             5.0  ì˜¤ëŠ˜ì˜ ë©”ë‰´ ë„ˆë¬´ ì¢‹ì•„ìš”. ì–´ì©œ ê°–ê°€ì§€ ë‹­ìš”ë¦¬ë¥¼ ê·¸ë ‡ê²Œ ì˜í•˜ì‹œì£ ?!! ì´ëŸ° ë¦¬ë·° ì˜...    1.0\n",
       "4             1.0                                  ì—¬ê¸°ì„œ ë°¥ë¨¹ê³  ë‘˜ë‹¤ ì¥ì—¼ê±¸ë¦¼;;    0.0\n",
       "...           ...                                                ...    ...\n",
       "2196          5.0  ìŒì‹ì´ ê°€ê²©ëŒ€ë¹„ ì–‘ë„ ë§ê³  ë§›ìˆëŠ” ê³³ì…ë‹ˆë‹¤!!!ë¶„ìœ„ê¸°ë‘ ì§ì›ë“¤ì˜ ë°˜ì‘ì´ ì¤‘ìš”í•˜ì‹  ë¶„...    1.0\n",
       "2197          1.0  ì—¬ê¸° ì•Œë°”ë“¤ì´ ì„¤ì³ëŒ€ëŠ”ê²Œ ì•„ë‹ˆë¼ë©´ ë°©ì†¡ íƒ”ì—ˆë‹¤ê³  ì´ë¦¬ ë¦¬ë·°ê°€ ë§ì€ê²ƒì¸ê°€ìš”? ì§„ì‹¬ í‰...    0.0\n",
       "2198          2.0        ìŒ. ì›¨ì´íŒ… ì‹­ì˜¤ë¶„í–ˆëŠ”ë° ì°¨ê²Œ ì‹ì€ ì¹˜ì¦ˆê°€ìŠ¤ê°€ ë‚˜ì™”ë‹¤. ì™•í˜¼ê°€ì¸ ë„ ê·¸ëƒ¥ ì˜ì˜.    0.0\n",
       "2199          5.0      ê°€ê²©ì€ ì–‘ì—ë¹„í•´ì„œ ì‹¸ê±°ë‚˜ ì ë‹¹í•˜ë‹¤ê³  ë´…ë‹ˆë‹¤. ê¸°ë³¸ ìƒëˆê¹ŒìŠ¤ëŠ” ì •ë§ ì–‘ì´ ë§ìŠ´ë‹ˆë‹¤.    1.0\n",
       "2200          4.0                                           20160515    1.0\n",
       "\n",
       "[2201 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hong3=hong[hong['label']==3].index.tolist()\n",
    "hong.drop(hong3,axis=0,inplace=True)\n",
    "hong=hong.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kakao_content</th>\n",
       "      <th>kakao_star</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì°¨ëŒì­ˆê¾¸ë¯¸ ìƒˆë¡œìƒê²¨ì„œ ë¨¹ì–´ë´¤ëŠ”ë”” ì •ë§ë§›ìˆì–´ìš”ì¡´ë§›íƒ±êµ¬ë¦¬êµ¬ë¦¬ ì˜¤ëŠ˜ë„ ì•„ì£¼ ì˜ë¨¹êµ¬ê°‘ë‹ˆë‹¤ ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì§„ì§œ ë§›ì§‘ ê·¼ì²˜ ë‹¤ë¥¸ í•´ë¬¼ì°œë“¤ ë¨¹ë‹¤ê°€ ê°€ê²Œê°€ ë³´ì—¬ì„œ ë“¤ì–´ì™€ ë´¤ëŠ”ë° ì§„ì§œ ë§›ìˆë„¤ìš”ë²Œì¨...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì²˜ìŒ ë°©ë¬¸í–ˆëŠ”ë° ìƒê°ë³´ë‹¤ ë§ì´ ë‹¬ë¼ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤ ì²œì•ˆë³¸ì ì—ëŠ” ë°‘ë°˜ì°¬ë§Œ 6ê°€ì§€ ì •ë„...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì§€ê¸ˆê» ë¨¹ì–´ë³¸ í•´ë¬¼ì°œê³¼ëŠ” ì •ë§ ë‹¤ë¥´ê²Œ ë„ˆë¬´ ë§›ìˆì–´ì„œ ë¨¹ê³  í¬ì¥ê¹Œì§€ í•´ì™”ìŠµë‹ˆë‹¤ìƒˆìš°ì°œë„...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ìƒê³ ê¸° ë§› ì¢‹ì•„ì„œ ì¶”ì²œí•©ë‹ˆë‹¤</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>ìŒì‹ë¨¹ê³  ê¸°ë¶„ë‚˜ì˜ê¸°ëŠ” ì²˜ìŒì´ë„¤ìš” ì¼ë‹¨ ì‚¬ì¥ì¸ì§€ ì¢…ì—…ì›ì¸ì§€ ì„œë¹„ìŠ¤ë§ˆì¸ë“œ ì•„ì£¼ ì €ê¸‰í•˜ë„¤...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>ì—¬ê¸° ë„ˆë¬´ë§›ìˆìŒ 1ì¸ë¶„ì— 200gì”© ì£¼ê³  ì°Œê°œë„ ë“¬ë¿ì„ ã…‹ã…‹ì‹ ì„ í•˜ê²Œ ì •ìœ¡ì ì—ì„œ ë°”ë¡œ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>í™˜ê¸°ë§Œ ì˜ ë˜ë©´ì™„ë²½í•œ ì‚¼ê²¹ì‚´ì§‘</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>ê³ ê¸°ë„ ê´œì°®ê³  íŠ¹íˆ ìŒˆì•¼ì±„ ë“¬ë¿ ì£¼ì…”ì„œ ë„˜ ì¢‹ë„¤ìš”ê·¼ë° ì™œ í‰ì í…ŒëŸ¬ë¥¼ í•œ ê±°ì§€</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>ë§›ìˆìŒ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         kakao_content  kakao_star  label\n",
       "0    ì°¨ëŒì­ˆê¾¸ë¯¸ ìƒˆë¡œìƒê²¨ì„œ ë¨¹ì–´ë´¤ëŠ”ë”” ì •ë§ë§›ìˆì–´ìš”ì¡´ë§›íƒ±êµ¬ë¦¬êµ¬ë¦¬ ì˜¤ëŠ˜ë„ ì•„ì£¼ ì˜ë¨¹êµ¬ê°‘ë‹ˆë‹¤ ...         5.0    1.0\n",
       "1    ì§„ì§œ ë§›ì§‘ ê·¼ì²˜ ë‹¤ë¥¸ í•´ë¬¼ì°œë“¤ ë¨¹ë‹¤ê°€ ê°€ê²Œê°€ ë³´ì—¬ì„œ ë“¤ì–´ì™€ ë´¤ëŠ”ë° ì§„ì§œ ë§›ìˆë„¤ìš”ë²Œì¨...         5.0    1.0\n",
       "2    ì²˜ìŒ ë°©ë¬¸í–ˆëŠ”ë° ìƒê°ë³´ë‹¤ ë§ì´ ë‹¬ë¼ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤ ì²œì•ˆë³¸ì ì—ëŠ” ë°‘ë°˜ì°¬ë§Œ 6ê°€ì§€ ì •ë„...         1.0    0.0\n",
       "3    ì§€ê¸ˆê» ë¨¹ì–´ë³¸ í•´ë¬¼ì°œê³¼ëŠ” ì •ë§ ë‹¤ë¥´ê²Œ ë„ˆë¬´ ë§›ìˆì–´ì„œ ë¨¹ê³  í¬ì¥ê¹Œì§€ í•´ì™”ìŠµë‹ˆë‹¤ìƒˆìš°ì°œë„...         5.0    1.0\n",
       "4                                      ìƒê³ ê¸° ë§› ì¢‹ì•„ì„œ ì¶”ì²œí•©ë‹ˆë‹¤         5.0    1.0\n",
       "..                                                 ...         ...    ...\n",
       "960  ìŒì‹ë¨¹ê³  ê¸°ë¶„ë‚˜ì˜ê¸°ëŠ” ì²˜ìŒì´ë„¤ìš” ì¼ë‹¨ ì‚¬ì¥ì¸ì§€ ì¢…ì—…ì›ì¸ì§€ ì„œë¹„ìŠ¤ë§ˆì¸ë“œ ì•„ì£¼ ì €ê¸‰í•˜ë„¤...         1.0    0.0\n",
       "961  ì—¬ê¸° ë„ˆë¬´ë§›ìˆìŒ 1ì¸ë¶„ì— 200gì”© ì£¼ê³  ì°Œê°œë„ ë“¬ë¿ì„ ã…‹ã…‹ì‹ ì„ í•˜ê²Œ ì •ìœ¡ì ì—ì„œ ë°”ë¡œ...         5.0    1.0\n",
       "962                                   í™˜ê¸°ë§Œ ì˜ ë˜ë©´ì™„ë²½í•œ ì‚¼ê²¹ì‚´ì§‘         5.0    1.0\n",
       "963         ê³ ê¸°ë„ ê´œì°®ê³  íŠ¹íˆ ìŒˆì•¼ì±„ ë“¬ë¿ ì£¼ì…”ì„œ ë„˜ ì¢‹ë„¤ìš”ê·¼ë° ì™œ í‰ì í…ŒëŸ¬ë¥¼ í•œ ê±°ì§€         5.0    1.0\n",
       "964                                                ë§›ìˆìŒ         5.0    1.0\n",
       "\n",
       "[965 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kakao3=kakao[kakao['label']==3].index.tolist()\n",
    "kakao.drop(kakao3,axis=0,inplace=True)\n",
    "kakao=kakao.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrs=rr[rr['label']==3.5].index.tolist()\n",
    "rr.drop(rrs,axis=0,inplace=True)\n",
    "rr=rr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hong=hong.drop('user_rating',axis=1)\n",
    "kakao=kakao.drop('kakao_star',axis=1)\n",
    "rr=rr.drop('4',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¼ë²¨ëª… ë°”ê¾¸ê¸°\n",
    "hong.rename(columns={'comment':'review'},inplace=True)\n",
    "kakao.rename(columns={'kakao_content':'review'},inplace=True)\n",
    "rr.rename(columns={'ë§›ìˆëŠ”ë° ê³ ê¸° ì–‘ì´ ì¡°ê¸ˆ ì ì„ ìˆ˜ë„.':'review'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì˜¤ë˜ í•´ì£¼ì„¸ìš”ğŸ™</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì˜¤ëŠ˜ì˜ ë©”ë‰´ ë„ˆë¬´ ì¢‹ì•„ìš”. ì–´ì©œ ê°–ê°€ì§€ ë‹­ìš”ë¦¬ë¥¼ ê·¸ë ‡ê²Œ ì˜í•˜ì‹œì£ ?!! ì´ëŸ° ë¦¬ë·° ì˜...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì—¬ê¸°ì„œ ë°¥ë¨¹ê³  ë‘˜ë‹¤ ì¥ì—¼ê±¸ë¦¼;;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0                                         â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸    1.0\n",
       "1                                           ì˜¤ë˜ í•´ì£¼ì„¸ìš”ğŸ™    1.0\n",
       "2       ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!    1.0\n",
       "3  ì˜¤ëŠ˜ì˜ ë©”ë‰´ ë„ˆë¬´ ì¢‹ì•„ìš”. ì–´ì©œ ê°–ê°€ì§€ ë‹­ìš”ë¦¬ë¥¼ ê·¸ë ‡ê²Œ ì˜í•˜ì‹œì£ ?!! ì´ëŸ° ë¦¬ë·° ì˜...    1.0\n",
       "4                                  ì—¬ê¸°ì„œ ë°¥ë¨¹ê³  ë‘˜ë‹¤ ì¥ì—¼ê±¸ë¦¼;;    0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([hong,kakao,food_review,thoure,rr],axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14227 entries, 0 to 14226\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   review  14227 non-null  object \n",
      " 1   label   14227 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 222.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ë¦¬ë·°_pre_trained.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-addons) (23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\user\\OneDrive\\ë°”íƒ•í™”~1-DESKTOP-R00ORLS-348\\ìº¡ìŠ¤í†¤\\KoBERT ê°ì„±ë¶„ë¥˜\\ë¦¬ë·°_pre_trained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14227 entries, 0 to 14226\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   review  14227 non-null  object \n",
      " 1   label   14227 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 222.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    12279\n",
       "0.0     1948\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14227, 2)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.863077\n",
       "0.0    0.136923\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íƒ€ê²Ÿë ˆì´ë¸” ë¶„í¬ í™•ì¸ íƒ€ê²Ÿë ˆì´ë¸” ë¶„í¬ í™•ì¸\n",
    "df['label'].value_counts(normalize=True)\n",
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13515, 2) (712, 2)\n"
     ]
    }
   ],
   "source": [
    "#í›ˆë ¨ë°ì´í„°ì™€ ê²€ì¦ë°ì´í„°ë¡œ ë¶„ë¦¬\n",
    "\n",
    "train,test=train_test_split(df,\n",
    "                            random_state=0,\n",
    "                            test_size=0.05,\n",
    "                            stratify=df['label'])\n",
    "\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.reset_index(drop=True)\n",
    "test=test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11664\n",
       "0     1851\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    615\n",
       "0     97\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
    "                     \"vocab_txt\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = u'â–'\n",
    "\n",
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "        SentencePiece based tokenizer. Peculiarities:\n",
    "            - requires `SentencePiece `_\n",
    "    \"\"\"\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_file,\n",
    "            vocab_txt,\n",
    "            do_lower_case=False,\n",
    "            remove_space=True,\n",
    "            keep_accents=False,\n",
    "            unk_token=\"[UNK]\",\n",
    "            sep_token=\"[SEP]\",\n",
    "            pad_token=\"[PAD]\",\n",
    "            cls_token=\"[CLS]\",\n",
    "            mask_token=\"[MASK]\",\n",
    "            **kwargs):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        self.max_len_single_sentence = self.model_max_length - 2  # take into account special tokens\n",
    "        self.max_len_sentences_pair = self.model_max_length - 3  # take into account special tokens\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize('NFKD', outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
    "        \"\"\" Tokenize a string. \"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        if not sample:\n",
    "            pieces = self.sp_model.EncodeAsPieces(text)\n",
    "        else:\n",
    "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index, return_unicode=True):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A RoBERTa sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A BERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "            to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer_78b3253a26.model from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\tokenizer_78b3253a26.model\n",
      "loading file vocab.txt from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"monologg/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "#kobert í† í¬ë‚˜ì´ì¦ˆ ì„í¬íŠ¸\n",
    "\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 517, 6983, 3298, 5585, 3466, 3742, 6821, 6827, 7275, 5, 1458, 6164, 7141, 5439, 2207, 7395, 4928, 7792, 1562, 1967, 7141, 5703, 5, 5, 3924, 781, 5400, 6999, 5, 3]\n",
      "['â–', 'ì™€', 'â–ì—¬', 'ê¸¸', 'â–ì™œ', 'â–ì´ì œ', 'ì•Œ', 'ì•˜', 'ì£ ', '!', 'â–ë„ˆë¬´', 'ë§›', 'ìˆ', 'ê³ ', 'â–ë°˜', 'ì°¬', 'â–í•˜ë‚˜', 'í•˜ë‚˜', 'â–ë‹¤', 'â–ë§›', 'ìˆ', 'ë„¤ìš”', '!', '!', 'â–ìì£¼', 'â–ê°ˆ', 'ê²Œ', 'ìš”', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!\t\"))\n",
    "print(tokenizer.tokenize(\"ì™€ ì—¬ê¸¸ ì™œ ì´ì œì•Œì•˜ì£ ! ë„ˆë¬´ë§›ìˆê³  ë°˜ì°¬ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ë§›ìˆë„¤ìš”!! ìì£¼ ê°ˆê²Œìš”!\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13515 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13515/13515 [00:04<00:00, 3198.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "    \n",
    "    SEQ_LEN = 64 #SEQ_LEN : ë²„íŠ¸ì— ë“¤ì–´ê°ˆ ì¸í’‹ì˜ ê¸¸ì´\n",
    "    \n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        # token : ë¬¸ì¥ì„ í† í°í™”í•¨\n",
    "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "       \n",
    "        # ë§ˆìŠ¤í¬ëŠ” í† í°í™”í•œ ë¬¸ì¥ì—ì„œ íŒ¨ë”©ì´ ì•„ë‹Œ ë¶€ë¶„ì€ 1, íŒ¨ë”©ì¸ ë¶€ë¶„ì€ 0ìœ¼ë¡œ í†µì¼\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "        \n",
    "        # ë¬¸ì¥ì˜ ì „í›„ê´€ê³„ë¥¼ êµ¬ë¶„í•´ì£¼ëŠ” ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë¬¸ì¥ì´ 1ê°œë°–ì— ì—†ìœ¼ë¯€ë¡œ ëª¨ë‘ 0\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        # ë²„íŠ¸ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” token, mask, segmentë¥¼ tokens, segmentsì— ê°ê° ì €ì¥\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        segments.append(segment)\n",
    "        \n",
    "        # ì •ë‹µ(ê¸ì • : 1 ë¶€ì • 0)ì„ targets ë³€ìˆ˜ì— ì €ì¥í•´ ì¤Œ\n",
    "        targets.append(data_df[LABEL_COLUMN][i])\n",
    "\n",
    "    # tokens, masks, segments, ì •ë‹µ ë³€ìˆ˜ targetsë¥¼ numpy arrayë¡œ ì§€ì •    \n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return [tokens, masks, segments], targets\n",
    "\n",
    "# ìœ„ì— ì •ì˜í•œ convert_data í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜\n",
    "def load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "    return data_x, data_y\n",
    "\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "# ê¸ë¶€ì • ë¬¸ì¥ì„ í¬í•¨í•˜ê³  ìˆëŠ” ì¹¼ëŸ¼\n",
    "DATA_COLUMN = \"review\"\n",
    "# ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ë¥¼ (1=ê¸ì •,0=ë¶€ì •) í¬í•¨í•˜ê³  ìˆëŠ” ì¹¼ëŸ¼\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "# train ë°ì´í„°ë¥¼ ë²„íŠ¸ ì¸í’‹ì— ë§ê²Œ ë³€í™˜\n",
    "train_x, train_y = load_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 712/712 [00:00<00:00, 2861.42it/s]\n"
     ]
    }
   ],
   "source": [
    "test_x,test_y=load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y = tf.expand_dims(train_y, axis=-1)\n",
    "#test_y = tf.expand_dims(test_y, axis=-1)\n",
    "train_y = np.expand_dims(train_y, axis=1)\n",
    "test_y = np.expand_dims(test_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13515,)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoBERT ê°ì„±ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\pytorch_model.bin\n",
      "Loading PyTorch weights from C:\\Users\\user\\.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\pytorch_model.bin\n",
      "PyTorch checkpoint contains 92,186,880 parameters\n",
      "Loaded 92,186,880 parameters in the TF 2.0 model.\n",
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
    "\n",
    "# í† í° ì¸í’‹, ë§ˆìŠ¤í¬ ì¸í’‹, ì„¸ê·¸ë¨¼íŠ¸ ì¸í’‹ ì •ì˜\n",
    "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
    "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
    "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
    "\n",
    "# ì¸í’‹ì´ [í† í°, ë§ˆìŠ¤í¬, ì„¸ê·¸ë¨¼íŠ¸]ì¸ ëª¨ë¸ ì •ì˜\n",
    "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs=bert_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model_2')>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 768])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\tensorflow_addons\\optimizers\\rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Rectified Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# ì´ batch size * 4 epoch = 2344 * 4\n",
    "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n",
    "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n",
    "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
    "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_masks (InputLayer)       [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_segment (InputLayer)     [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  92186880    ['input_word_ids[0][0]',         \n",
      "                                thPoolingAndCrossAt               'input_masks[0][0]',            \n",
      "                                tentions(last_hidde               'input_segment[0][0]']          \n",
      "                                n_state=(None, 64,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_120 (Dropout)          (None, 768)          0           ['tf_bert_model_2[0][1]']        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            769         ['dropout_120[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 92,187,649\n",
      "Trainable params: 92,187,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "212/212 [==============================] - 2490s 12s/step - loss: 0.2269 - accuracy: 0.9217 - val_loss: 0.2376 - val_accuracy: 0.9199\n",
      "Epoch 2/2\n",
      "212/212 [==============================] - 2379s 11s/step - loss: 0.1902 - accuracy: 0.9383 - val_loss: 0.2665 - val_accuracy: 0.9143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ea45279d0>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.fit(train_x,train_y,epochs=2,shuffle=True,batch_size=64,validation_data=(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_model.save('KoBERT_model')\n",
    "#sentiment_model.save_weights('saveweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_convert_data(data_df):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "\n",
    "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        tokens.append(token)\n",
    "        segments.append(segment)\n",
    "        masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "# ìœ„ì— ì •ì˜í•œ convert_data í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜\n",
    "def predict_load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_x = predict_convert_data(data_df)\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/712 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 712/712 [00:00<00:00, 2728.20it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = predict_load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 42s 2s/step\n"
     ]
    }
   ],
   "source": [
    "preds = sentiment_model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 1)\n",
      "(712,)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1\n",
       "6      1\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     1\n",
       "13     0\n",
       "14     1\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18     1\n",
       "19     0\n",
       "20     1\n",
       "21     1\n",
       "22     1\n",
       "23     1\n",
       "24     1\n",
       "25     1\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     0\n",
       "30     1\n",
       "31     0\n",
       "32     1\n",
       "33     1\n",
       "34     1\n",
       "35     1\n",
       "36     0\n",
       "37     0\n",
       "38     0\n",
       "39     1\n",
       "40     1\n",
       "41     1\n",
       "42     1\n",
       "43     1\n",
       "44     1\n",
       "45     1\n",
       "46     1\n",
       "47     1\n",
       "48     1\n",
       "49     1\n",
       "50     1\n",
       "51     0\n",
       "52     1\n",
       "53     1\n",
       "54     0\n",
       "55     0\n",
       "56     1\n",
       "57     1\n",
       "58     1\n",
       "59     1\n",
       "60     1\n",
       "61     1\n",
       "62     1\n",
       "63     1\n",
       "64     0\n",
       "65     1\n",
       "66     1\n",
       "67     1\n",
       "68     0\n",
       "69     1\n",
       "70     1\n",
       "71     1\n",
       "72     1\n",
       "73     1\n",
       "74     1\n",
       "75     1\n",
       "76     1\n",
       "77     1\n",
       "78     1\n",
       "79     1\n",
       "80     1\n",
       "81     0\n",
       "82     1\n",
       "83     1\n",
       "84     1\n",
       "85     1\n",
       "86     1\n",
       "87     1\n",
       "88     1\n",
       "89     1\n",
       "90     1\n",
       "91     1\n",
       "92     1\n",
       "93     1\n",
       "94     1\n",
       "95     1\n",
       "96     1\n",
       "97     1\n",
       "98     0\n",
       "99     1\n",
       "100    1\n",
       "101    1\n",
       "102    1\n",
       "103    0\n",
       "104    0\n",
       "105    1\n",
       "106    1\n",
       "107    1\n",
       "108    1\n",
       "109    1\n",
       "110    1\n",
       "111    1\n",
       "112    1\n",
       "113    1\n",
       "114    1\n",
       "115    1\n",
       "116    0\n",
       "117    1\n",
       "118    1\n",
       "119    1\n",
       "120    1\n",
       "121    1\n",
       "122    1\n",
       "123    1\n",
       "124    1\n",
       "125    1\n",
       "126    1\n",
       "127    1\n",
       "128    0\n",
       "129    0\n",
       "130    0\n",
       "131    1\n",
       "132    1\n",
       "133    1\n",
       "134    1\n",
       "135    1\n",
       "136    1\n",
       "137    1\n",
       "138    1\n",
       "139    1\n",
       "140    1\n",
       "141    1\n",
       "142    1\n",
       "143    1\n",
       "144    1\n",
       "145    1\n",
       "146    1\n",
       "147    0\n",
       "148    1\n",
       "149    0\n",
       "150    1\n",
       "151    1\n",
       "152    1\n",
       "153    1\n",
       "154    0\n",
       "155    1\n",
       "156    1\n",
       "157    1\n",
       "158    1\n",
       "159    1\n",
       "160    1\n",
       "161    1\n",
       "162    1\n",
       "163    1\n",
       "164    1\n",
       "165    1\n",
       "166    1\n",
       "167    1\n",
       "168    1\n",
       "169    0\n",
       "170    1\n",
       "171    1\n",
       "172    1\n",
       "173    1\n",
       "174    1\n",
       "175    1\n",
       "176    1\n",
       "177    1\n",
       "178    1\n",
       "179    1\n",
       "180    1\n",
       "181    1\n",
       "182    1\n",
       "183    1\n",
       "184    1\n",
       "185    0\n",
       "186    1\n",
       "187    1\n",
       "188    1\n",
       "189    1\n",
       "190    1\n",
       "191    1\n",
       "192    1\n",
       "193    1\n",
       "194    1\n",
       "195    1\n",
       "196    1\n",
       "197    1\n",
       "198    1\n",
       "199    1\n",
       "200    0\n",
       "201    1\n",
       "202    1\n",
       "203    1\n",
       "204    1\n",
       "205    0\n",
       "206    1\n",
       "207    1\n",
       "208    1\n",
       "209    1\n",
       "210    1\n",
       "211    1\n",
       "212    1\n",
       "213    1\n",
       "214    1\n",
       "215    0\n",
       "216    1\n",
       "217    1\n",
       "218    1\n",
       "219    1\n",
       "220    1\n",
       "221    1\n",
       "222    0\n",
       "223    1\n",
       "224    1\n",
       "225    1\n",
       "226    1\n",
       "227    1\n",
       "228    1\n",
       "229    1\n",
       "230    1\n",
       "231    1\n",
       "232    1\n",
       "233    1\n",
       "234    1\n",
       "235    1\n",
       "236    1\n",
       "237    1\n",
       "238    1\n",
       "239    1\n",
       "240    1\n",
       "241    1\n",
       "242    0\n",
       "243    1\n",
       "244    1\n",
       "245    0\n",
       "246    1\n",
       "247    1\n",
       "248    1\n",
       "249    1\n",
       "250    1\n",
       "251    1\n",
       "252    1\n",
       "253    1\n",
       "254    1\n",
       "255    1\n",
       "256    1\n",
       "257    1\n",
       "258    1\n",
       "259    1\n",
       "260    1\n",
       "261    1\n",
       "262    1\n",
       "263    1\n",
       "264    1\n",
       "265    1\n",
       "266    1\n",
       "267    1\n",
       "268    1\n",
       "269    1\n",
       "270    1\n",
       "271    1\n",
       "272    1\n",
       "273    1\n",
       "274    1\n",
       "275    1\n",
       "276    1\n",
       "277    1\n",
       "278    1\n",
       "279    1\n",
       "280    1\n",
       "281    0\n",
       "282    1\n",
       "283    1\n",
       "284    0\n",
       "285    1\n",
       "286    1\n",
       "287    1\n",
       "288    1\n",
       "289    1\n",
       "290    1\n",
       "291    1\n",
       "292    1\n",
       "293    1\n",
       "294    1\n",
       "295    1\n",
       "296    1\n",
       "297    1\n",
       "298    1\n",
       "299    1\n",
       "300    1\n",
       "301    1\n",
       "302    1\n",
       "303    1\n",
       "304    1\n",
       "305    1\n",
       "306    1\n",
       "307    0\n",
       "308    1\n",
       "309    1\n",
       "310    1\n",
       "311    0\n",
       "312    0\n",
       "313    1\n",
       "314    0\n",
       "315    1\n",
       "316    1\n",
       "317    1\n",
       "318    0\n",
       "319    0\n",
       "320    1\n",
       "321    1\n",
       "322    0\n",
       "323    1\n",
       "324    1\n",
       "325    1\n",
       "326    1\n",
       "327    1\n",
       "328    1\n",
       "329    0\n",
       "330    1\n",
       "331    1\n",
       "332    1\n",
       "333    1\n",
       "334    1\n",
       "335    1\n",
       "336    1\n",
       "337    1\n",
       "338    1\n",
       "339    0\n",
       "340    0\n",
       "341    1\n",
       "342    1\n",
       "343    0\n",
       "344    0\n",
       "345    1\n",
       "346    1\n",
       "347    1\n",
       "348    1\n",
       "349    1\n",
       "350    1\n",
       "351    1\n",
       "352    1\n",
       "353    1\n",
       "354    1\n",
       "355    1\n",
       "356    1\n",
       "357    1\n",
       "358    1\n",
       "359    1\n",
       "360    1\n",
       "361    1\n",
       "362    0\n",
       "363    1\n",
       "364    1\n",
       "365    1\n",
       "366    1\n",
       "367    1\n",
       "368    1\n",
       "369    1\n",
       "370    1\n",
       "371    1\n",
       "372    1\n",
       "373    1\n",
       "374    1\n",
       "375    1\n",
       "376    1\n",
       "377    1\n",
       "378    1\n",
       "379    1\n",
       "380    1\n",
       "381    1\n",
       "382    1\n",
       "383    1\n",
       "384    1\n",
       "385    1\n",
       "386    1\n",
       "387    1\n",
       "388    1\n",
       "389    1\n",
       "390    1\n",
       "391    1\n",
       "392    1\n",
       "393    1\n",
       "394    1\n",
       "395    1\n",
       "396    1\n",
       "397    1\n",
       "398    1\n",
       "399    0\n",
       "400    1\n",
       "401    1\n",
       "402    1\n",
       "403    1\n",
       "404    1\n",
       "405    1\n",
       "406    1\n",
       "407    1\n",
       "408    1\n",
       "409    1\n",
       "410    1\n",
       "411    1\n",
       "412    1\n",
       "413    1\n",
       "414    1\n",
       "415    1\n",
       "416    1\n",
       "417    1\n",
       "418    0\n",
       "419    1\n",
       "420    1\n",
       "421    0\n",
       "422    1\n",
       "423    1\n",
       "424    1\n",
       "425    1\n",
       "426    1\n",
       "427    1\n",
       "428    1\n",
       "429    1\n",
       "430    1\n",
       "431    1\n",
       "432    1\n",
       "433    1\n",
       "434    0\n",
       "435    1\n",
       "436    1\n",
       "437    1\n",
       "438    1\n",
       "439    0\n",
       "440    1\n",
       "441    1\n",
       "442    1\n",
       "443    0\n",
       "444    1\n",
       "445    1\n",
       "446    1\n",
       "447    0\n",
       "448    1\n",
       "449    1\n",
       "450    1\n",
       "451    1\n",
       "452    1\n",
       "453    1\n",
       "454    1\n",
       "455    1\n",
       "456    1\n",
       "457    0\n",
       "458    1\n",
       "459    1\n",
       "460    1\n",
       "461    1\n",
       "462    1\n",
       "463    1\n",
       "464    1\n",
       "465    0\n",
       "466    1\n",
       "467    1\n",
       "468    1\n",
       "469    1\n",
       "470    1\n",
       "471    1\n",
       "472    1\n",
       "473    1\n",
       "474    1\n",
       "475    1\n",
       "476    1\n",
       "477    1\n",
       "478    1\n",
       "479    1\n",
       "480    1\n",
       "481    1\n",
       "482    1\n",
       "483    1\n",
       "484    1\n",
       "485    1\n",
       "486    1\n",
       "487    1\n",
       "488    1\n",
       "489    0\n",
       "490    1\n",
       "491    1\n",
       "492    1\n",
       "493    1\n",
       "494    1\n",
       "495    1\n",
       "496    1\n",
       "497    0\n",
       "498    1\n",
       "499    1\n",
       "500    0\n",
       "501    1\n",
       "502    1\n",
       "503    1\n",
       "504    0\n",
       "505    0\n",
       "506    1\n",
       "507    1\n",
       "508    0\n",
       "509    1\n",
       "510    1\n",
       "511    1\n",
       "512    1\n",
       "513    1\n",
       "514    1\n",
       "515    1\n",
       "516    1\n",
       "517    0\n",
       "518    1\n",
       "519    1\n",
       "520    1\n",
       "521    1\n",
       "522    1\n",
       "523    1\n",
       "524    0\n",
       "525    1\n",
       "526    1\n",
       "527    1\n",
       "528    1\n",
       "529    0\n",
       "530    1\n",
       "531    1\n",
       "532    1\n",
       "533    0\n",
       "534    0\n",
       "535    1\n",
       "536    1\n",
       "537    1\n",
       "538    0\n",
       "539    1\n",
       "540    1\n",
       "541    1\n",
       "542    1\n",
       "543    1\n",
       "544    0\n",
       "545    1\n",
       "546    1\n",
       "547    1\n",
       "548    1\n",
       "549    1\n",
       "550    1\n",
       "551    1\n",
       "552    1\n",
       "553    0\n",
       "554    1\n",
       "555    1\n",
       "556    1\n",
       "557    1\n",
       "558    1\n",
       "559    1\n",
       "560    1\n",
       "561    1\n",
       "562    1\n",
       "563    1\n",
       "564    1\n",
       "565    1\n",
       "566    1\n",
       "567    0\n",
       "568    1\n",
       "569    1\n",
       "570    1\n",
       "571    0\n",
       "572    1\n",
       "573    1\n",
       "574    1\n",
       "575    1\n",
       "576    1\n",
       "577    1\n",
       "578    1\n",
       "579    1\n",
       "580    0\n",
       "581    1\n",
       "582    1\n",
       "583    1\n",
       "584    1\n",
       "585    1\n",
       "586    0\n",
       "587    1\n",
       "588    1\n",
       "589    1\n",
       "590    0\n",
       "591    1\n",
       "592    1\n",
       "593    1\n",
       "594    1\n",
       "595    0\n",
       "596    1\n",
       "597    1\n",
       "598    1\n",
       "599    1\n",
       "600    0\n",
       "601    1\n",
       "602    1\n",
       "603    1\n",
       "604    1\n",
       "605    1\n",
       "606    0\n",
       "607    1\n",
       "608    1\n",
       "609    1\n",
       "610    1\n",
       "611    1\n",
       "612    0\n",
       "613    0\n",
       "614    1\n",
       "615    1\n",
       "616    1\n",
       "617    1\n",
       "618    1\n",
       "619    1\n",
       "620    1\n",
       "621    1\n",
       "622    1\n",
       "623    1\n",
       "624    1\n",
       "625    0\n",
       "626    1\n",
       "627    1\n",
       "628    1\n",
       "629    1\n",
       "630    1\n",
       "631    1\n",
       "632    0\n",
       "633    1\n",
       "634    0\n",
       "635    1\n",
       "636    1\n",
       "637    1\n",
       "638    1\n",
       "639    0\n",
       "640    0\n",
       "641    1\n",
       "642    1\n",
       "643    1\n",
       "644    1\n",
       "645    1\n",
       "646    1\n",
       "647    1\n",
       "648    1\n",
       "649    0\n",
       "650    1\n",
       "651    1\n",
       "652    1\n",
       "653    1\n",
       "654    1\n",
       "655    1\n",
       "656    1\n",
       "657    1\n",
       "658    1\n",
       "659    0\n",
       "660    1\n",
       "661    1\n",
       "662    1\n",
       "663    1\n",
       "664    1\n",
       "665    1\n",
       "666    1\n",
       "667    0\n",
       "668    1\n",
       "669    1\n",
       "670    1\n",
       "671    1\n",
       "672    1\n",
       "673    1\n",
       "674    1\n",
       "675    1\n",
       "676    1\n",
       "677    1\n",
       "678    0\n",
       "679    1\n",
       "680    1\n",
       "681    1\n",
       "682    1\n",
       "683    1\n",
       "684    0\n",
       "685    0\n",
       "686    1\n",
       "687    1\n",
       "688    1\n",
       "689    1\n",
       "690    1\n",
       "691    1\n",
       "692    1\n",
       "693    1\n",
       "694    1\n",
       "695    0\n",
       "696    1\n",
       "697    1\n",
       "698    0\n",
       "699    1\n",
       "700    0\n",
       "701    0\n",
       "702    1\n",
       "703    1\n",
       "704    1\n",
       "705    0\n",
       "706    0\n",
       "707    0\n",
       "708    1\n",
       "709    1\n",
       "710    1\n",
       "711    1\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.49      0.61        97\n",
      "           1       0.92      0.98      0.95       615\n",
      "\n",
      "    accuracy                           0.91       712\n",
      "   macro avg       0.86      0.74      0.78       712\n",
      "weighted avg       0.91      0.91      0.91       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = test['label']\n",
    "# F1 Score í™•ì¸\n",
    "print(classification_report(y_true, np.round(preds,0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ì œ ë°ì´í„° ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    token = tokenizer.encode(data, max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "    \n",
    "    num_zeros = token.count(0) \n",
    "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros \n",
    "    segment = [0]*SEQ_LEN\n",
    "\n",
    "    tokens.append(token)\n",
    "    segments.append(segment)\n",
    "    masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "def review_evaluation_predict(sentence):\n",
    "    data_x = sentence_convert_data(sentence)\n",
    "    predict = sentiment_model.predict(data_x)\n",
    "    predict_value = np.ravel(predict)\n",
    "    predict_answer = np.round(predict_value,0).item()\n",
    "    \n",
    "    if predict_answer == 0:\n",
    "      print(\"(ë¶€ì • í™•ë¥  : %.2f) ë¶€ì •ì ì¸ ë¦¬ë·°ì…ë‹ˆë‹¤.\" % (1-predict_value))\n",
    "    elif predict_answer == 1:\n",
    "      print(\"(ê¸ì • í™•ë¥  : %.2f) ê¸ì •ì ì¸ ë¦¬ë·°ì…ë‹ˆë‹¤.\" % predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 220ms/step\n",
      "(ë¶€ì • í™•ë¥  : 0.93) ë¶€ì •ì ì¸ ë¦¬ë·°ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "review_evaluation_predict(\"ë‹¤ì‹œëŠ” ê°€ê³  ì‹¶ì§€ ì•Šì•„ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 194ms/step\n",
      "(ê¸ì • í™•ë¥  : 0.98) ê¸ì •ì ì¸ ë¦¬ë·°ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "review_evaluation_predict(\"ë§›ì€ ì—†ëŠ”ë° ì‹ë‹¹ ë‚´ë¶€ ë¶„ìœ„ê¸°ëŠ” ì¢‹ì•„ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 211ms/step\n",
      "(ê¸ì • í™•ë¥  : 0.98) ê¸ì •ì ì¸ ë¦¬ë·°ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "review_evaluation_predict(\"ë§›ìˆì–´ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    token = tokenizer.encode(data, max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "    \n",
    "    num_zeros = token.count(0) \n",
    "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros \n",
    "    segment = [0]*SEQ_LEN\n",
    "\n",
    "    tokens.append(token)\n",
    "    segments.append(segment)\n",
    "    masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "def review_evaluation_predict(sentence):\n",
    "    data_x = sentence_convert_data(sentence)\n",
    "    predict = sentiment_model.predict(data_x)\n",
    "    predict_value = np.ravel(predict)\n",
    "    predict_answer = np.round(predict_value,0).item()\n",
    "    \n",
    "    if predict_answer == 0:\n",
    "      return \"(ë¶€ì • í™•ë¥  : %.2f) ë¶€ì •\" % (1-predict_value)\n",
    "    elif predict_answer == 1:\n",
    "      return \"(ê¸ì • í™•ë¥  : %.2f) ê¸ì •\" % predict_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ìƒê°€ì—…ì†Œë²ˆí˜¸</th>\n",
       "      <th>ê°€ê²Œëª…</th>\n",
       "      <th>ì›ë˜ì´ë¦„</th>\n",
       "      <th>ë¦¬ë·°</th>\n",
       "      <th>ë¶„ë¦¬ë¦¬ë·°</th>\n",
       "      <th>ë¦¬ë·°ë‚ ì§œ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20592276</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...</td>\n",
       "      <td>ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20592276</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...</td>\n",
       "      <td>ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ì¡°ê¸ˆ ìŠ´ìŠ´í•œ í¸ì¸ë° ë§›ì´ ê½¤ í›Œë¥­í•˜ë‹¤.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20592276</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...</td>\n",
       "      <td>ë©´ë°œì€ í•˜ëŠ˜í•˜ëŠ˜ ë¶€ë“œëŸ¬ìš´ í¸ì´ë‹¤.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20592276</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...</td>\n",
       "      <td>í…Œì´ë¸” ê°„ê²©ì´ ì¢ê³  ì„œë²„ ì´ëª¨ë‹˜ë“¤ì´ ì‚´ì§ ë¶ˆì¹œì ˆí•œ í¸ì´ë¼ ì˜¨ì „íˆ ìœ ëª…ì„¸ì˜ ë§›ê²½í—˜ì„ ...</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20592276</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ê¹¡í†µë§Œë‘</td>\n",
       "      <td>ì£¼ë§ì— ë°©ë¬¸í•˜ë©´ ì›¨ì´íŒ…ì´ ê¹ë‹ˆë‹¤. í•œì‹œê°„ì •ë„ ëŒ€ê¸°í•˜ë©´ ë“œì‹¤ ìˆ˜ ìˆì–´ìš”. ë§Œë‘ê°€ ê¹”ë”...</td>\n",
       "      <td>ì£¼ë§ì— ë°©ë¬¸í•˜ë©´ ì›¨ì´íŒ…ì´ ê¹ë‹ˆë‹¤.</td>\n",
       "      <td>2023-2-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ìƒê°€ì—…ì†Œë²ˆí˜¸   ê°€ê²Œëª…  ì›ë˜ì´ë¦„                                                 ë¦¬ë·°  \\\n",
       "0  20592276  ê¹¡í†µë§Œë‘  ê¹¡í†µë§Œë‘  ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...   \n",
       "1  20592276  ê¹¡í†µë§Œë‘  ê¹¡í†µë§Œë‘  ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...   \n",
       "2  20592276  ê¹¡í†µë§Œë‘  ê¹¡í†µë§Œë‘  ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...   \n",
       "3  20592276  ê¹¡í†µë§Œë‘  ê¹¡í†µë§Œë‘  ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ...   \n",
       "4  20592276  ê¹¡í†µë§Œë‘  ê¹¡í†µë§Œë‘  ì£¼ë§ì— ë°©ë¬¸í•˜ë©´ ì›¨ì´íŒ…ì´ ê¹ë‹ˆë‹¤. í•œì‹œê°„ì •ë„ ëŒ€ê¸°í•˜ë©´ ë“œì‹¤ ìˆ˜ ìˆì–´ìš”. ë§Œë‘ê°€ ê¹”ë”...   \n",
       "\n",
       "                                                ë¶„ë¦¬ë¦¬ë·°       ë¦¬ë·°ë‚ ì§œ  \n",
       "0                       ë§Œë‘ê°€ ì˜›ë‚  ì§‘ì—ì„œ ë¹šì€ ë”± ê·¸ ë§›ì´ì–´ì„œ, ì¢‹ì•˜ë‹¤.   2023-3-9  \n",
       "1            ì¹¼ë§Œë‘êµ­ì„ ë¨¹ì—ˆëŠ”ë° ì‚¬ê³¨ë² ì´ìŠ¤ì— ì¡°ê¸ˆ ìŠ´ìŠ´í•œ í¸ì¸ë° ë§›ì´ ê½¤ í›Œë¥­í•˜ë‹¤.   2023-3-9  \n",
       "2                                 ë©´ë°œì€ í•˜ëŠ˜í•˜ëŠ˜ ë¶€ë“œëŸ¬ìš´ í¸ì´ë‹¤.   2023-3-9  \n",
       "3  í…Œì´ë¸” ê°„ê²©ì´ ì¢ê³  ì„œë²„ ì´ëª¨ë‹˜ë“¤ì´ ì‚´ì§ ë¶ˆì¹œì ˆí•œ í¸ì´ë¼ ì˜¨ì „íˆ ìœ ëª…ì„¸ì˜ ë§›ê²½í—˜ì„ ...   2023-3-9  \n",
       "4                                 ì£¼ë§ì— ë°©ë¬¸í•˜ë©´ ì›¨ì´íŒ…ì´ ê¹ë‹ˆë‹¤.  2023-2-19  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\user\\OneDrive\\ë°”íƒ•í™”~1-DESKTOP-R00ORLS-348\\ìº¡ìŠ¤í†¤\\data_git\\data\\diningcode_data\\ë‹¤ì´ë‹ì½”ë“œ ë¶ì´Œ ë¦¬ë·° ë¶„ë¦¬.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n"
     ]
    }
   ],
   "source": [
    "data['evaluation'] = data['ë¶„ë¦¬ë¦¬ë·°'].apply(review_evaluation_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('test_review_predict.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('test_review_predict.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
